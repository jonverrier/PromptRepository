<!-- Generated by StrongAIAutoDoc 20260219 -->

This library provides a provider-agnostic chat and embedding layer for applications that need LLM interactions. It exposes unified drivers, factories, and prompt utilities so callers can generate text, stream responses, invoke tools/functions, enforce JSON schemas, and compute embeddings. The system dynamically routes to OpenAI, Azure OpenAI, or Google Gemini based on configuration and environment. It also supports attachments (inline or provider-hosted files) and simple prompt repositories, including JSON-backed templates. External dependencies include LLM provider APIs (Responses/Generative, Files, Embeddings), local prompt JSON files, and environment variables for API keys, endpoints, and provider defaults.

```mermaid
C4Context
title LLM Chat & Embedding Library - Context Diagram

Person(dev, "App Developer / Calling Service", "Uses the library to call LLMs and embeddings")

System_Boundary(sys, "LLM Chat & Embedding Library") {
  System(factory, "Factories", "ChatFactory / ChatWithAttachmentFactory / EmbedFactory\nSelect providers and instantiate drivers")
  System(chat, "Chat Drivers", "OpenAI / AzureOpenAI (generic Responses) and Google Gemini chat")
  System(attach, "Chat With Attachment", "OpenAI / Azure OpenAI (Files + Responses), Google Gemini (inline)")
  System(embed, "Embedding Drivers", "OpenAI and Azure OpenAI embeddings + cosineSimilarity")
  System(prompts, "Prompt Repository", "In-memory and file-based prompt templates")
  System(helpers, "Driver Helpers", "Retry, exponential backoff, error normalization")
  System(entry, "Core Types & Contracts", "Enums, interfaces, function/tool schemas")
}

System_Ext(openai, "OpenAI API", "Responses, Files, Embeddings")
System_Ext(azure, "Azure OpenAI", "Responses, Files, Embeddings")
System_Ext(gemini, "Google Gemini API", "Generative Language API")
System_Ext(promptfile, "Prompt JSON File", "prompts.json on disk")
System_Ext(env, "Environment / Config", "API keys, endpoints, NODE_ENV")

Rel(dev, factory, "Creates drivers via", "EModel, EModelProvider")
Rel(factory, chat, "Instantiates")
Rel(factory, attach, "Instantiates")
Rel(factory, embed, "Instantiates")
Rel(chat, openai, "Responses API")
Rel(chat, azure, "Responses API")
Rel(chat, gemini, "Generate / Stream")
Rel(attach, openai, "Files upload/delete + Responses")
Rel(attach, azure, "Files upload/delete + Responses")
Rel(attach, gemini, "Inline attachment via base64")
Rel(embed, openai, "Embeddings API")
Rel(embed, azure, "Embeddings API")
Rel(prompts, promptfile, "Loads templates from")
Rel(factory, env, "Reads provider defaults (NODE_ENV)")
Rel(chat, env, "Reads API keys, endpoints")
Rel(attach, env, "Reads API keys, endpoints")
Rel(embed, env, "Reads API keys, endpoints")
Rel(helpers, openai, "Retries with backoff on transient errors", "HTTP 429/5xx")
Rel(helpers, azure, "Retries with backoff on transient errors", "HTTP 429/5xx")
Rel(helpers, gemini, "Retries with backoff on transient errors", "HTTP 429/5xx")
```

Key components and external interactions:
- Factories centralize provider selection. They choose OpenAI, Azure OpenAI, or Gemini using requested provider and NODE_ENV, returning the appropriate chat, attachment-capable, or embedding driver.
- Chat Drivers unify non-streaming, streaming, tool-calling, and schema-constrained responses. OpenAI and Azure use the Responses API; Gemini uses its Generative Language API with function-calling loops and forced-tools emulation.
- Chat With Attachment Drivers handle inline files or provider file references. OpenAI/Azure use Files + Responses; Gemini embeds base64 data inline. Optional deletion cleans up uploaded files.
- Embedding Drivers call OpenAI or Azure Embeddings and expose cosine similarity.
- Prompt Repository loads/expands templates from in-memory or prompts.json, validating parameters.
- Driver Helpers implement resilient retries, exponential backoff, and error normalization across all provider calls, improving reliability under rate limits and transient failures.